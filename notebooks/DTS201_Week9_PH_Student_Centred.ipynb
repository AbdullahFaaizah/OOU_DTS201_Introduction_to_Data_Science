{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sakinat-Folorunso/OOU_DTS201_Introduction_to_Data_Science/blob/main/notebooks/DTS201_Week9_PH_Student_Centred.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6002a781",
      "metadata": {
        "id": "6002a781"
      },
      "source": [
        "# DTS 201 â€“ INTRODUCTION TO DATA SCIENCE  \n",
        "## WEEK 9 PRACTICAL (PH): BIG DATA, LARGE FILES & WORKING IN JUPYTER / GOOGLE COLAB\n",
        "\n",
        "**Instructor:** DR SAKINAT FOLORRUNSO â€“ ASSOCIATE PROFESSOR OF AI SYSTEMS AND FAIR DATA  \n",
        "**Department:** COMPUTER SCIENCES, OLABISI ONABANJO UNIVERSITY, AGO-IWOYE, OGUN STATE, NIGERIA  \n",
        "\n",
        "---\n",
        "\n",
        "### ðŸŽ¯ Practical Learning Objectives (Week 9)\n",
        "\n",
        "By the end of this practical session, you should be able to:\n",
        "\n",
        "- Explain the basic idea of **Big Data** and why large files are challenging.  \n",
        "- Describe key features of **Jupyter Notebooks** and **Google Colab** as data science tools.  \n",
        "- Simulate a **large dataset** and inspect its memory usage.  \n",
        "- Load and process data using **chunking** (`chunksize`) instead of reading everything at once.  \n",
        "- Apply simple analytics (e.g. computing totals and averages) on large files in a **memory-efficient** way.  \n",
        "\n",
        "> ðŸ’¡ This lab is a gentle introduction to â€œbig data thinkingâ€ using **simulated large CSV files** rather than real multi-GB datasets, to keep it safe for student laptops and lab machines.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c4f6849b",
      "metadata": {
        "id": "c4f6849b"
      },
      "source": [
        "## 1. What Do We Mean by â€œBig Dataâ€?\n",
        "\n",
        "In practice, **Big Data** often means data that is:\n",
        "\n",
        "- Too **large** to fit into memory (RAM) on a single machine.  \n",
        "- Too **fast** (high-velocity streams) to process one-by-one in real time without special tools.  \n",
        "- Too **complex** (variety of formats: text, images, logs, clicks, sensors) to handle with simple spreadsheets.\n",
        "\n",
        "In this course, we will **not** work with terabytes of data, but we will:\n",
        "\n",
        "- Think about how **file size** and **memory** affect analysis.  \n",
        "- Practise techniques like **chunking** and **type optimisation**.  \n",
        "- Use tools like **Jupyter** and **Google Colab** effectively.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b67237bf",
      "metadata": {
        "id": "b67237bf"
      },
      "source": [
        "### 1.1 Jupyter Notebooks and Google Colab (Quick Orientation)\n",
        "\n",
        "Both **Jupyter Notebooks** and **Google Colab** allow you to:\n",
        "\n",
        "- Write and run code in small blocks called **cells**.  \n",
        "- Mix **code**, **output**, and **explanatory text** (markdown) in one document.  \n",
        "- Save, share, and re-run experiments easily.  \n",
        "\n",
        "Key cell types:\n",
        "\n",
        "- **Code cells** â€“ contain Python code that you can run.  \n",
        "- **Markdown cells** â€“ contain formatted text for explanations, comments, and notes.\n",
        "\n",
        "> âœï¸ **Student Task:**  \n",
        "> - Click on a markdown cell in this notebook and try editing some text.  \n",
        "> - Run the cell (Shift + Enter) to see how the formatting updates.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5831256",
      "metadata": {
        "id": "f5831256"
      },
      "source": [
        "### 1.2 Optional: Mount Google Drive (If Using Google Colab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7b50449",
      "metadata": {
        "id": "f7b50449"
      },
      "outputs": [],
      "source": [
        "# Run this cell ONLY if:\n",
        "# 1. You are working in Google Colab, AND\n",
        "# 2. Your data files are stored in your Google Drive.\n",
        "\n",
        "# Import the drive module from google.colab to allow mounting Google Drive\n",
        "from google.colab import drive  # Provides tools to connect Colab to your Google Drive\n",
        "\n",
        "# Mount your Google Drive so that it appears as a folder in Colab\n",
        "drive.mount('/content/drive')  # Follow the instructions and paste the authorization code when asked\n",
        "\n",
        "# After mounting, you can access files under '/content/drive/MyDrive/'\n",
        "# Example path: '/content/drive/MyDrive/Datasets/large_file.csv'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a471bdb1",
      "metadata": {
        "id": "a471bdb1"
      },
      "source": [
        "## 2. Importing Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3819d4b4",
      "metadata": {
        "id": "3819d4b4"
      },
      "outputs": [],
      "source": [
        "# Import pandas for working with tabular data (rows and columns)\n",
        "import pandas as pd\n",
        "\n",
        "# Import numpy for numerical operations and random number generation\n",
        "import numpy as np\n",
        "\n",
        "# Import matplotlib and seaborn for simple plots (optional for this lab)\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Ensure plots appear inside the notebook\n",
        "%matplotlib inline\n",
        "\n",
        "# Set a visual style for plots\n",
        "plt.style.use(\"seaborn-v0_8\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b22d8592",
      "metadata": {
        "id": "b22d8592"
      },
      "source": [
        "## 3. Simulating a Large Dataset in Memory"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "befe0499",
      "metadata": {
        "id": "befe0499"
      },
      "source": [
        "To avoid downloading very large files, we will **simulate** a dataset with many rows.\n",
        "\n",
        "We create a DataFrame with, for example:\n",
        "\n",
        "- An `id` column (1, 2, 3, â€¦).  \n",
        "- A `category` column (e.g. A, B, C).  \n",
        "- A numerical column like `value` (random numbers).  \n",
        "- A `timestamp` column (simulated dates).  \n",
        "\n",
        "You can adjust the number of rows to fit your machine.\n",
        "\n",
        "> âš ï¸ **Warning:** Do not make this too large on a weak machine (e.g. avoid 5 million rows on an old laptop).  \n",
        "> Start modestly (e.g. 100,000 rows) and only increase if your system is responsive.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9834116",
      "metadata": {
        "id": "b9834116"
      },
      "outputs": [],
      "source": [
        "# Choose how many rows to simulate\n",
        "# Start with 100_000 (one hundred thousand). You can try increasing to 500_000 or 1_000_000 later.\n",
        "n_rows = 100_000  # TODO: adjust based on your machine's capacity\n",
        "\n",
        "# Set a random seed so results are reproducible\n",
        "np.random.seed(42)  # Ensures that random numbers are the same each time you run the cell\n",
        "\n",
        "# Create a sequence of integer IDs from 1 to n_rows\n",
        "ids = np.arange(1, n_rows + 1)  # Unique identifier for each row\n",
        "\n",
        "# Create a categorical column with a few repeating categories\n",
        "categories = np.random.choice(['A', 'B', 'C', 'D'], size=n_rows)  # Randomly assign A/B/C/D\n",
        "\n",
        "# Create a numerical column of random values (e.g. from a normal distribution)\n",
        "values = np.random.normal(loc=100, scale=15, size=n_rows)  # Mean 100, standard deviation 15\n",
        "\n",
        "# Create a column of random dates within a given range\n",
        "dates = pd.to_datetime('2024-01-01') + pd.to_timedelta(\n",
        "    np.random.randint(0, 365, size=n_rows), unit='D'\n",
        ")  # Random dates in 2024\n",
        "\n",
        "# Combine into a single DataFrame\n",
        "df_large = pd.DataFrame({\n",
        "    'id': ids,\n",
        "    'category': categories,\n",
        "    'value': values,\n",
        "    'date': dates\n",
        "})\n",
        "\n",
        "# Display the first few rows to confirm the structure\n",
        "df_large.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cbf8be8f",
      "metadata": {
        "id": "cbf8be8f"
      },
      "source": [
        "> âœï¸ **Student Task 1:**  \n",
        "> - Change `n_rows` to a smaller value (e.g. 10_000) and then to a larger value (e.g. 300_000 if your machine can handle it).  \n",
        "> - Observe whether your notebook becomes slower to respond when working with larger n_rows.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "036b8b8e",
      "metadata": {
        "id": "036b8b8e"
      },
      "source": [
        "### 3.1 Checking Memory Usage of the DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d397ee1d",
      "metadata": {
        "id": "d397ee1d"
      },
      "outputs": [],
      "source": [
        "# Use memory_usage to see how much memory each column consumes\n",
        "memory_per_column = df_large.memory_usage(deep=True)  # deep=True gives a more accurate measurement for object columns\n",
        "\n",
        "# Display memory usage per column (in bytes)\n",
        "memory_per_column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2144d1b9",
      "metadata": {
        "id": "2144d1b9"
      },
      "outputs": [],
      "source": [
        "# Compute total memory usage in megabytes (MB)\n",
        "total_memory_bytes = memory_per_column.sum()  # Sum memory used by all columns\n",
        "total_memory_mb = total_memory_bytes / (1024 ** 2)  # Convert bytes to megabytes\n",
        "\n",
        "print(f\"Total memory usage of df_large: {total_memory_mb:.2f} MB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41267909",
      "metadata": {
        "id": "41267909"
      },
      "source": [
        "> âœï¸ **Student Reflection 2:**  \n",
        "> - How much memory (in MB) does your `df_large` use?  \n",
        "> - What happens to the memory usage if you double `n_rows`? Does it roughly double as well?  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "65b609a8",
      "metadata": {
        "id": "65b609a8"
      },
      "source": [
        "## 4. Saving the Large Dataset to CSV (Simulating a Large File)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a4d8809",
      "metadata": {
        "id": "8a4d8809"
      },
      "source": [
        "To simulate working with a **large file from disk**, we will:\n",
        "\n",
        "1. Save `df_large` to a CSV file.  \n",
        "2. Pretend that we are now another analyst loading this file in **chunks** instead of reading everything at once.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "389a9e88",
      "metadata": {
        "id": "389a9e88"
      },
      "outputs": [],
      "source": [
        "# Specify a file name for saving the large dataset\n",
        "large_csv_path = \"simulated_large_dataset.csv\"  # This file will be created in the current working directory\n",
        "\n",
        "# Save the DataFrame to CSV without the index column\n",
        "df_large.to_csv(large_csv_path, index=False)  # Writes the entire DataFrame to a CSV file\n",
        "\n",
        "print(f\"Saved large dataset to: {large_csv_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae3ef011",
      "metadata": {
        "id": "ae3ef011"
      },
      "source": [
        "## 5. Loading and Processing the Large CSV File in Chunks"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8df928a8",
      "metadata": {
        "id": "8df928a8"
      },
      "source": [
        "When files become too large to load entirely into memory, we can use **chunking**, where:\n",
        "\n",
        "- The file is read in **small portions** (e.g. 10,000 rows at a time).  \n",
        "- Each chunk is processed, and partial results are **aggregated**.  \n",
        "\n",
        "We will compute the **overall mean** and **sum** of the `value` column using chunks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65345c08",
      "metadata": {
        "id": "65345c08"
      },
      "outputs": [],
      "source": [
        "# Choose a chunk size (number of rows per chunk)\n",
        "chunk_size = 20_000  # Adjust based on your machine; smaller chunks use less memory\n",
        "\n",
        "# Initialize variables to accumulate results\n",
        "total_sum = 0.0         # Will hold the sum of 'value' over entire file\n",
        "total_count = 0         # Will hold the total number of rows\n",
        "\n",
        "# Use pandas.read_csv with chunksize to iterate over the file\n",
        "for chunk in pd.read_csv(large_csv_path, chunksize=chunk_size):\n",
        "    # For each chunk, compute the sum and count of the 'value' column\n",
        "    chunk_sum = chunk['value'].sum()     # Sum of 'value' in this chunk\n",
        "    chunk_count = chunk['value'].count() # Number of non-missing entries in this chunk\n",
        "\n",
        "    # Update total accumulators\n",
        "    total_sum += chunk_sum\n",
        "    total_count += chunk_count\n",
        "\n",
        "# After processing all chunks, compute the overall mean\n",
        "overall_mean = total_sum / total_count\n",
        "\n",
        "print(\"Processed file in chunks of size:\", chunk_size)\n",
        "print(\"Total rows processed:\", total_count)\n",
        "print(\"Overall mean of 'value' (computed with chunks):\", overall_mean)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d71f53be",
      "metadata": {
        "id": "d71f53be"
      },
      "outputs": [],
      "source": [
        "# For comparison, compute the mean directly from df_large in memory\n",
        "direct_mean = df_large['value'].mean()\n",
        "\n",
        "print(\"Direct mean from in-memory DataFrame:\", direct_mean)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f83e904",
      "metadata": {
        "id": "1f83e904"
      },
      "source": [
        "> âœï¸ **Student Task 3:**  \n",
        "> - Compare the chunk-based overall mean and the direct in-memory mean. Are they (almost) the same?  \n",
        "> - Change `chunk_size` to a smaller number (e.g. 5_000) and re-run. Did the result change? Did the process become slower or faster?  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7bd9000c",
      "metadata": {
        "id": "7bd9000c"
      },
      "source": [
        "## 6. Grouped Aggregation in Chunks (By Category)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b097ef91",
      "metadata": {
        "id": "b097ef91"
      },
      "source": [
        "We can also perform **grouped aggregation** (e.g. average value per category) while reading the file in chunks.\n",
        "\n",
        "This is more realistic, as many big data tasks require grouping by category, date, or region.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1bc3dd82",
      "metadata": {
        "id": "1bc3dd82"
      },
      "outputs": [],
      "source": [
        "# Initialize an empty dictionary to accumulate sums and counts by category\n",
        "category_sums = {}   # Will map category -> sum of 'value'\n",
        "category_counts = {} # Will map category -> count of 'value'\n",
        "\n",
        "# Iterate over the CSV file in chunks again\n",
        "for chunk in pd.read_csv(large_csv_path, chunksize=chunk_size):\n",
        "    # Group the chunk by 'category' and compute sum and count for 'value'\n",
        "    grouped = chunk.groupby('category')['value'].agg(['sum', 'count'])\n",
        "\n",
        "    # Update the global accumulators for each category\n",
        "    for cat, row in grouped.iterrows():\n",
        "        # Get current sum and count for this category from the dictionaries (default 0 if not present)\n",
        "        category_sums[cat] = category_sums.get(cat, 0.0) + row['sum']\n",
        "        category_counts[cat] = category_counts.get(cat, 0) + row['count']\n",
        "\n",
        "# After processing all chunks, compute mean value per category\n",
        "category_means = {cat: category_sums[cat] / category_counts[cat] for cat in category_sums.keys()}\n",
        "\n",
        "# Convert the result into a DataFrame for display\n",
        "category_summary = pd.DataFrame({\n",
        "    'category': list(category_means.keys()),\n",
        "    'mean_value': list(category_means.values())\n",
        "})\n",
        "\n",
        "# Display the summary\n",
        "category_summary"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5ef961c",
      "metadata": {
        "id": "a5ef961c"
      },
      "source": [
        "> âœï¸ **Student Task 4:**  \n",
        "> - Which category has the highest mean value? Which has the lowest?  \n",
        "> - How could such grouped statistics be useful in a business or scientific context (e.g. comparing branches, products, or patient groups)?  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75cb3c61",
      "metadata": {
        "id": "75cb3c61"
      },
      "source": [
        "## 7. Reducing Memory Usage with Data Type Optimisation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1da3f865",
      "metadata": {
        "id": "1da3f865"
      },
      "source": [
        "One important technique with large data is to **reduce memory usage** by selecting more efficient data types.\n",
        "\n",
        "Examples:\n",
        "\n",
        "- Using `int32` instead of `int64` when values are small.  \n",
        "- Using `float32` instead of `float64` when full precision is not necessary.  \n",
        "- Converting repeated text fields to **categorical** type.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe862844",
      "metadata": {
        "id": "fe862844"
      },
      "outputs": [],
      "source": [
        "# Show original data types\n",
        "print(\"Original dtypes:\")\n",
        "print(df_large.dtypes)\n",
        "\n",
        "# Create a copy of the DataFrame for optimisation\n",
        "df_optimized = df_large.copy()\n",
        "\n",
        "# Convert 'id' from default (int64) to int32 to save memory\n",
        "df_optimized['id'] = df_optimized['id'].astype('int32')\n",
        "\n",
        "# Convert 'category' from object (string) to category type\n",
        "df_optimized['category'] = df_optimized['category'].astype('category')\n",
        "\n",
        "# Convert 'value' from float64 to float32\n",
        "df_optimized['value'] = df_optimized['value'].astype('float32')\n",
        "\n",
        "# Note: 'date' can remain as datetime64[ns]\n",
        "\n",
        "# Check new data types\n",
        "print(\"\n",
        "Optimized dtypes:\")\n",
        "print(df_optimized.dtypes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62382e0e",
      "metadata": {
        "id": "62382e0e"
      },
      "outputs": [],
      "source": [
        "# Compare memory usage before and after optimisation\n",
        "original_mem_mb = df_large.memory_usage(deep=True).sum() / (1024 ** 2)\n",
        "optimized_mem_mb = df_optimized.memory_usage(deep=True).sum() / (1024 ** 2)\n",
        "\n",
        "print(f\"Original memory usage: {original_mem_mb:.2f} MB\")\n",
        "print(f\"Optimized memory usage: {optimized_mem_mb:.2f} MB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9cdeb25",
      "metadata": {
        "id": "f9cdeb25"
      },
      "source": [
        "> âœï¸ **Student Reflection 5:**  \n",
        "> - How much memory did you save (in MB and as a percentage) after optimisation?  \n",
        "> - Why is this kind of optimisation important when working with much larger datasets (e.g. millions of rows, many columns)?  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70e270f4",
      "metadata": {
        "id": "70e270f4"
      },
      "source": [
        "## 8. Mini Big Data Exercise"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50973e6e",
      "metadata": {
        "id": "50973e6e"
      },
      "source": [
        "> ðŸ§ª **Student Activity:**  \n",
        "> Using the techniques in this notebook, complete the following:\n",
        "\n",
        "1. Change `n_rows` to simulate a dataset that is **large but still manageable** on your machine.  \n",
        "2. Save it to CSV and process it in chunks to compute:  \n",
        ">    - Overall mean and standard deviation of `value`.  \n",
        ">    - Mean value by category.  \n",
        "3. Optimise dtypes and report:  \n",
        ">    - Original memory usage.  \n",
        ">    - Optimised memory usage.  \n",
        "4. Write a short paragraph describing:  \n",
        ">    - What challenges you observed when increasing the dataset size.  \n",
        ">    - How chunking and type optimisation can help in real big data scenarios.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c2cb9fc0",
      "metadata": {
        "id": "c2cb9fc0"
      },
      "source": [
        "## 9. Summary and Reflection (Week 9 PH)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac3c6f89",
      "metadata": {
        "id": "ac3c6f89"
      },
      "source": [
        "In this Week 9 practical, you have:\n",
        "\n",
        "- Reviewed the concept of **Big Data** and its challenges for memory and computation.  \n",
        "- Explored the use of **Jupyter/Colab** notebooks for interactive data science.  \n",
        "- Simulated a **large dataset** and examined its memory usage.  \n",
        "- Practised reading and processing a large CSV file in **chunks** using `pandas.read_csv(chunksize=...)`.  \n",
        "- Performed global and grouped aggregations on chunked data.  \n",
        "- Reduced memory footprint using **data type optimisation**.\n",
        "\n",
        "> ðŸ§  **Final Reflection (1â€“2 paragraphs):**  \n",
        "> - How do todayâ€™s activities change your thinking about working with â€œbigâ€ datasets in practice?  \n",
        "> - What limitations do Jupyter/Colab still have when dealing with truly massive data (terabytes or more)?  \n",
        "> - Which of the techniques (chunking, dtypes, simulation) do you think you will use again in future projects?  \n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}